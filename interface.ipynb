{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dad53b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docling in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.41.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (2.10.5)\n",
      "Requirement already satisfied: docling-core<3.0.0,>=2.42.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling-core[chunking]<3.0.0,>=2.42.0->docling) (2.42.0)\n",
      "Requirement already satisfied: docling-parse<5.0.0,>=4.0.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (4.1.0)\n",
      "Requirement already satisfied: docling-ibm-models<4,>=3.6.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (3.8.1)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (1.2.0)\n",
      "Requirement already satisfied: pypdfium2<5.0.0,>=4.30.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (4.30.1)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (2.10.1)\n",
      "Requirement already satisfied: huggingface_hub<1,>=0.23 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (0.33.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.2 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (2.32.3)\n",
      "Requirement already satisfied: easyocr<2.0,>=1.7 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (1.7.2)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (2024.12.14)\n",
      "Requirement already satisfied: rtree<2.0.0,>=1.3.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (1.4.0)\n",
      "Requirement already satisfied: typer<0.17.0,>=0.12.5 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (0.16.0)\n",
      "Requirement already satisfied: python-docx<2.0.0,>=1.1.2 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (1.2.0)\n",
      "Requirement already satisfied: python-pptx<2.0.0,>=1.0.2 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (1.0.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\louise\\appdata\\roaming\\python\\python311\\site-packages (from docling) (4.13.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (2.2.2)\n",
      "Requirement already satisfied: marko<3.0.0,>=2.1.2 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (2.1.4)\n",
      "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (3.1.5)\n",
      "Requirement already satisfied: lxml<6.0.0,>=4.0.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (5.3.0)\n",
      "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (11.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (4.67.1)\n",
      "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (1.6.0)\n",
      "Requirement already satisfied: pylatexenc<3.0,>=2.10 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (2.10)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling) (1.15.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\louise\\appdata\\roaming\\python\\python311\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.12.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in c:\\users\\louise\\appdata\\roaming\\python\\python311\\site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (4.23.0)\n",
      "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (1.1.0)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (0.9.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.1 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (6.0.2)\n",
      "Requirement already satisfied: latex2mathml<4.0.0,>=3.77.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (3.78.0)\n",
      "Requirement already satisfied: semchunk<3.0.0,>=2.2.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling-core[chunking]<3.0.0,>=2.42.0->docling) (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling-core[chunking]<3.0.0,>=2.42.0->docling) (4.48.0)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.2.2 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling-ibm-models<4,>=3.6.0->docling) (2.7.1)\n",
      "Requirement already satisfied: torchvision<1,>=0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling-ibm-models<4,>=3.6.0->docling) (0.22.1)\n",
      "Requirement already satisfied: jsonlines<4.0.0,>=3.1.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling-ibm-models<4,>=3.6.0->docling) (3.1.0)\n",
      "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.6.0.66 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling-ibm-models<4,>=3.6.0->docling) (4.12.0.88)\n",
      "Requirement already satisfied: safetensors<1,>=0.4.3 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from safetensors[torch]<1,>=0.4.3->docling-ibm-models<4,>=3.6.0->docling) (0.5.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.24.4 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docling-ibm-models<4,>=3.6.0->docling) (2.2.6)\n",
      "Requirement already satisfied: pywin32>=305 in c:\\users\\louise\\appdata\\roaming\\python\\python311\\site-packages (from docling-parse<5.0.0,>=4.0.0->docling) (308)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr<2.0,>=1.7->docling) (0.25.2)\n",
      "Requirement already satisfied: python-bidi in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr<2.0,>=1.7->docling) (0.6.6)\n",
      "Requirement already satisfied: Shapely in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr<2.0,>=1.7->docling) (2.0.7)\n",
      "Requirement already satisfied: pyclipper in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr<2.0,>=1.7->docling) (1.3.0.post6)\n",
      "Requirement already satisfied: ninja in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from easyocr<2.0,>=1.7->docling) (1.11.1.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1,>=0.23->docling) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1,>=0.23->docling) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub<1,>=0.23->docling) (24.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3.0.0,>=2.1.4->docling) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3.0.0,>=2.1.4->docling) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3.0.0,>=2.1.4->docling) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (1.1.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (0.4.1)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-pptx<2.0.0,>=1.0.2->docling) (3.2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.32.2->docling) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.32.2->docling) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.65.0->docling) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.17.0,>=0.12.5->docling) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.17.0,>=0.12.5->docling) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.17.0,>=0.12.5->docling) (14.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\louise\\appdata\\roaming\\python\\python311\\site-packages (from jsonlines<4.0.0,>=3.1.0->docling-ibm-models<4,>=3.6.0->docling) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\louise\\appdata\\roaming\\python\\python311\\site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\louise\\appdata\\roaming\\python\\python311\\site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\louise\\appdata\\roaming\\python\\python311\\site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.42.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (0.24.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\louise\\appdata\\roaming\\python\\python311\\site-packages (from rich>=10.11.0->typer<0.17.0,>=0.12.5->docling) (2.19.1)\n",
      "Requirement already satisfied: mpire[dill] in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (2.10.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (3.1.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (0.21.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2025.6.11)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (0.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.17.0,>=0.12.5->docling) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch<3.0.0,>=2.2.2->docling-ibm-models<4,>=3.6.0->docling) (3.0.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.15 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (0.70.18)\n",
      "Requirement already satisfied: dill>=0.4.0 in c:\\users\\louise\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.42.0->docling) (0.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5fb61e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' ï¿½ï¿½ï¿½ï¿½ï¿½Ú²ï¿½ï¿½ï¿½ï¿½â²¿ï¿½ï¿½ï¿½î£¬Ò²ï¿½ï¿½ï¿½Ç¿ï¿½ï¿½ï¿½ï¿½ÐµÄ³ï¿½ï¿½ï¿½\n",
      "ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ä¼ï¿½ï¿½ï¿½\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!apt-get -qq update && apt-get -qq install -y poppler-utils\n",
    "\n",
    "\n",
    "!pip install -q gradio azure-storage-blob pdf2image openai docling pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5cb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_script_with_N_detection_and_editable_UI.py\n",
    "# English comments throughout.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import base64\n",
    "import logging\n",
    "import tempfile\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "\n",
    "# PDF/table tools (you already used these in your original script)\n",
    "from pdf2image import convert_from_path\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# OpenAI client - placeholder, keep your existing initialization\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- CONFIG --- replace with your real values via environment variables\n",
    "AZURE_DEPLOYMENT = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # replace securely\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# ------------------------\n",
    "# Utilities\n",
    "# ------------------------\n",
    "def encode_image_to_base64(path: str) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode()\n",
    "\n",
    "def make_pdf_iframe(path: str):\n",
    "    b64 = base64.b64encode(open(path,\"rb\").read()).decode()\n",
    "    return f'<iframe src=\"data:application/pdf;base64,{b64}\" width=\"600\" height=\"800\" style=\"border:none;\"></iframe>'\n",
    "\n",
    "RE_MEAN_SD = re.compile(r'^\\s*([+-]?\\d+(?:\\.\\d+)?)\\s*(?:Â±|\\+/-|\\u00B1)\\s*([+-]?\\d+(?:\\.\\d+)?)\\s*$')\n",
    "RE_PAREN = re.compile(r'^\\s*([+-]?\\d+(?:\\.\\d+)?)\\s*\\(\\s*([+-]?\\d+(?:\\.\\d+)?)\\s*\\)\\s*$')\n",
    "\n",
    "def parse_mean_sd_cell(cell):\n",
    "    \"\"\"Try to parse 'mean Â± sd' or 'mean(sd)' patterns; else None.\"\"\"\n",
    "    if pd.isna(cell):\n",
    "        return None\n",
    "    s = str(cell).strip()\n",
    "    m = RE_MEAN_SD.match(s)\n",
    "    if m:\n",
    "        return float(m.group(1)), float(m.group(2))\n",
    "    m2 = RE_PAREN.match(s)\n",
    "    if m2:\n",
    "        return float(m2.group(1)), float(m2.group(2))\n",
    "    return None\n",
    "\n",
    "# ------------------------\n",
    "# f_range and ANOVA-from-summaries (simplified & robust)\n",
    "# (We implement a version suited to 1D (k groups) or 2D (a x b) cells.)\n",
    "# ------------------------\n",
    "def _anova_oneway_from_summaries(m, s, n):\n",
    "    m = np.asarray(m, dtype=float)\n",
    "    s = np.asarray(s, dtype=float)\n",
    "    n = np.asarray(n, dtype=float)\n",
    "    k = len(m)\n",
    "    N = np.sum(n)\n",
    "    if k < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    grand_mean = np.sum(n * m) / N\n",
    "    ss_between = np.sum(n * (m - grand_mean) ** 2)\n",
    "    ss_within = np.sum((n - 1.0) * (s ** 2))\n",
    "    df_between = k - 1\n",
    "    df_within = int(np.sum(n) - k)\n",
    "    if df_between <= 0 or df_within <= 0:\n",
    "        return np.nan, df_between, df_within\n",
    "    ms_between = ss_between / df_between\n",
    "    ms_within = ss_within / df_within\n",
    "    F = ms_between / ms_within if ms_within != 0 else np.nan\n",
    "    return F, df_between, df_within\n",
    "\n",
    "def _anova_twoway_from_summaries(m, s, n):\n",
    "    m = np.asarray(m, dtype=float)\n",
    "    s = np.asarray(s, dtype=float)\n",
    "    n = np.asarray(n, dtype=float)\n",
    "    if m.ndim != 2:\n",
    "        raise ValueError(\"m must be 2D for two-way ANOVA\")\n",
    "    a, b = m.shape\n",
    "    N = np.sum(n)\n",
    "    grand_mean = np.sum(n * m) / N\n",
    "    ss_within = np.sum((n - 1.0) * (s ** 2))\n",
    "    ss_treatment = np.sum(n * (m - grand_mean) ** 2)\n",
    "    n_row = np.sum(n, axis=1)\n",
    "    mean_row = np.sum(n * m, axis=1) / n_row\n",
    "    n_col = np.sum(n, axis=0)\n",
    "    mean_col = np.sum(n * m, axis=0) / n_col\n",
    "    ss_A = np.sum(n_row * (mean_row - grand_mean) ** 2)\n",
    "    ss_B = np.sum(n_col * (mean_col - grand_mean) ** 2)\n",
    "    ss_AB = ss_treatment - ss_A - ss_B\n",
    "    df_A = a - 1\n",
    "    df_B = b - 1\n",
    "    df_AB = (a - 1) * (b - 1)\n",
    "    df_within = int(np.sum(n) - a * b)\n",
    "    if df_within <= 0:\n",
    "        return [np.nan, np.nan, np.nan], [df_A, df_B, df_AB, df_within]\n",
    "    ms_A = ss_A / df_A if df_A > 0 else np.nan\n",
    "    ms_B = ss_B / df_B if df_B > 0 else np.nan\n",
    "    ms_AB = ss_AB / df_AB if df_AB > 0 else np.nan\n",
    "    ms_within = ss_within / df_within\n",
    "    F_A = ms_A / ms_within if ms_within != 0 else np.nan\n",
    "    F_B = ms_B / ms_within if ms_within != 0 else np.nan\n",
    "    F_AB = ms_AB / ms_within if ms_within != 0 else np.nan\n",
    "    return [F_A, F_B, F_AB], [df_A, df_B, df_AB, df_within]\n",
    "\n",
    "def f_range(m, s, n, title=None, show_t=False, dp_p=-1, labels=None, max_enumeration=2**16):\n",
    "    \"\"\"\n",
    "    Compute nominal, min and max plausible F (or t if show_t=True) values\n",
    "    given reported means, sds, ns, allowing for rounding error.\n",
    "    Works for 1D (one-way) and 2D (two-way) cell arrays.\n",
    "    \"\"\"\n",
    "    m_arr = np.array(m)\n",
    "    s_arr = np.array(s)\n",
    "    n_arr = np.array(n)\n",
    "\n",
    "    # detect decimals\n",
    "    dp = dp_p\n",
    "    if dp_p == -1:\n",
    "        dp = 0\n",
    "        numbers = np.concatenate([np.ravel(m_arr).astype(float), np.ravel(s_arr).astype(float)])\n",
    "        for x in numbers:\n",
    "            if not np.isclose(x, np.round(x, 0)):\n",
    "                dp = max(dp, 1)\n",
    "                if not np.isclose(x * 10, np.round(x * 10, 0)):\n",
    "                    dp = max(dp, 2)\n",
    "    delta = (0.1 ** dp) / 2.0\n",
    "\n",
    "    # nominal\n",
    "    if m_arr.ndim == 1:\n",
    "        f_nom, _, _ = _anova_oneway_from_summaries(m_arr, s_arr, n_arr)\n",
    "        useFs = [f_nom]\n",
    "        default_labels = [\"F\" if not show_t else \"t\"]\n",
    "    elif m_arr.ndim == 2:\n",
    "        Fs_nom, _ = _anova_twoway_from_summaries(m_arr, s_arr, n_arr)\n",
    "        useFs = Fs_nom\n",
    "        default_labels = [\"A F\", \"B F\", \"A:B F\"]\n",
    "    else:\n",
    "        raise ValueError(\"m must be 1D or 2D array-like\")\n",
    "\n",
    "    m_flat = np.ravel(m_arr)\n",
    "    s_flat = np.ravel(s_arr)\n",
    "    n_flat = np.ravel(n_arr)\n",
    "    l = len(m_flat)\n",
    "\n",
    "    total_combinations = 2 ** l\n",
    "    if total_combinations > max_enumeration:\n",
    "        enumeration_mode = \"sampled\"\n",
    "        rng = np.random.default_rng(12345)\n",
    "        sampled_codes = rng.choice(total_combinations, size=max_enumeration, replace=False)\n",
    "    else:\n",
    "        enumeration_mode = \"full\"\n",
    "        sampled_codes = None\n",
    "\n",
    "    eps = 1e-8\n",
    "    s_hi = np.maximum(s_flat - delta, eps)\n",
    "    s_lo = s_flat + delta\n",
    "\n",
    "    f_hi = np.array(useFs, dtype=float)\n",
    "    f_lo = np.array(useFs, dtype=float)\n",
    "\n",
    "    def compute_for_signs(sign_vector):\n",
    "        m_adj = (m_flat + sign_vector)\n",
    "        if m_arr.ndim == 1:\n",
    "            F_hi, _, _ = _anova_oneway_from_summaries(m_adj, s_hi, n_flat)\n",
    "            F_lo, _, _ = _anova_oneway_from_summaries(m_adj, s_lo, n_flat)\n",
    "            return [F_hi], [F_lo]\n",
    "        else:\n",
    "            a, b = m_arr.shape\n",
    "            m_adj_mat = m_adj.reshape(a, b)\n",
    "            s_hi_mat = s_hi.reshape(a, b)\n",
    "            s_lo_mat = s_lo.reshape(a, b)\n",
    "            F_hi_list, _ = _anova_twoway_from_summaries(m_adj_mat, s_hi_mat, n_arr)\n",
    "            F_lo_list, _ = _anova_twoway_from_summaries(m_adj_mat, s_lo_mat, n_arr)\n",
    "            return F_hi_list, F_lo_list\n",
    "\n",
    "    if enumeration_mode == \"full\":\n",
    "        for signs in itertools.product([-delta, delta], repeat=l):\n",
    "            sign_vec = np.array(signs)\n",
    "            F_hi_list, F_lo_list = compute_for_signs(sign_vec)\n",
    "            f_hi = np.maximum(f_hi, np.array(F_hi_list, dtype=float))\n",
    "            f_lo = np.minimum(f_lo, np.array(F_lo_list, dtype=float))\n",
    "    else:\n",
    "        for code in sampled_codes:\n",
    "            sign_vector = np.array([delta if ((code >> i) & 1) == 1 else -delta for i in range(l)])\n",
    "            F_hi_list, F_lo_list = compute_for_signs(sign_vector)\n",
    "            f_hi = np.maximum(f_hi, np.array(F_hi_list, dtype=float))\n",
    "            f_lo = np.minimum(f_lo, np.array(F_lo_list, dtype=float))\n",
    "\n",
    "    if show_t:\n",
    "        f_nom_out = np.sqrt(np.clip(np.array(useFs, dtype=float), a_min=0.0, a_max=None))\n",
    "        f_hi = np.sqrt(np.clip(f_hi, a_min=0.0, a_max=None))\n",
    "        f_lo = np.sqrt(np.clip(f_lo, a_min=0.0, a_max=None))\n",
    "    else:\n",
    "        f_nom_out = np.array(useFs, dtype=float)\n",
    "\n",
    "    labels_out = labels if (labels is not None and len(labels) == len(f_nom_out)) else default_labels\n",
    "    result = {\n",
    "        \"title\": title,\n",
    "        \"labels\": labels_out,\n",
    "        \"nominal\": [None if np.isnan(x) else float(x) for x in f_nom_out],\n",
    "        \"min\": [None if np.isnan(x) else float(x) for x in f_lo],\n",
    "        \"max\": [None if np.isnan(x) else float(x) for x in f_hi],\n",
    "        \"dp\": int(dp),\n",
    "        \"enumeration_mode\": enumeration_mode,\n",
    "        \"total_combinations\": total_combinations\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def extract_ai(pdf_path):\n",
    "    pil_pages = convert_from_path(pdf_path, dpi=200)\n",
    "\n",
    "    image_blocks = []\n",
    "    for idx, img in enumerate(pil_pages, start=1):\n",
    "        tmp = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n",
    "        img.save(tmp.name, format=\"PNG\")\n",
    "        b64 = encode_image_to_base64(tmp.name)\n",
    "        os.unlink(tmp.name)\n",
    "        image_blocks.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/png;base64,{b64}\"}\n",
    "        })\n",
    "    messages = []\n",
    "    content = [\n",
    "            {\"type\": \"text\",\n",
    "             \"text\":f'''Please extract all exist tables from the following image and return them in json format with datas and headers in exactly same\n",
    "             format,make sure that all numerical values are extracted with full accuracy,do not change any format or name,include keys: table title and table note,is_baseline indicate whether the table is baseline table.\n",
    "             Example JSON output(please do not include this one):\n",
    "             ```json\n",
    "            {{\n",
    "            \"tables\": [\n",
    "                {{\n",
    "                \"table_title\": \"Patient Characteristics\",\n",
    "                \"is_baseline\": true,\n",
    "                \"group_ns\": {{ \"Study Group\": 50, \"Control Group\": 48 }}  // optional: integer N or null\n",
    "                \"headers\": [\"Feature\", \"Study Group\", \"Control Group\", \"P-value\"],\n",
    "                \"data\": [\n",
    "                    [\"Age (years)\", \"30 Â± 3.51\", \"29.2 Â± 2.93\", \".76\"],\n",
    "                    [\"Height (cm)\", \"158 Â± 12.33\", \"164 Â± 14.55\", \".54\"],\n",
    "                    [\"Body weight (kg)\", \"68 Â± 3.20\", \"65 Â± 4.21\", \".12\"]\n",
    "                ],\n",
    "                \"table_note\": \"Baseline characteristics of patients.\"\n",
    "                }},\n",
    "                {{\n",
    "                \"table_title\": \"Outcome 8 months after therapy\",\n",
    "                \"is_baseline\": false,\n",
    "                \"group_ns\": {{ \"Study Group\": 50, \"Control Group\": 48 }}  // optional: integer N or null\n",
    "                \"headers\": [\"Outcome\", \"Study Group\", \"Control Group\", \"P-value\"],\n",
    "                \"data\": [\n",
    "                    [\"Menstruating\", \"35 (89.6%)\", \"13 (33.3%)\", \"<.001*\"],\n",
    "                    [\"Ovulating\", \"27 (69.2%)\", \"10 (25.6%)\", \"<.001*\"],\n",
    "                    [\"POF\", \"4 (11.4%)\", \"21 (66.6%)\", \"<.001*\"]\n",
    "                ],\n",
    "                \"table_note\": \"P-value < .05 was considered statistically significant.\"\n",
    "                }}\n",
    "                ]\n",
    "            }}\n",
    "            ```\n",
    "             if no table,return No table provided:\\n\\n'''\n",
    "            }]+ image_blocks\n",
    "    messages.append({\"role\": 'user', \"content\": content})\n",
    "    # Call the API to generate the table extraction\n",
    "    completion = client.chat.completions.create(\n",
    "        model=AZURE_DEPLOYMENT,\n",
    "        messages=messages,\n",
    "        max_tokens=8000,\n",
    "        temperature=0.0,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    print(\"\\nðŸ“‹ Extracted Table:\\n\")\n",
    "    extract_tables = completion.choices[0].message.content\n",
    "    print(extract_tables)\n",
    "    chat_prompt_split=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a data analysis expert.\"},\n",
    "        {\"role\": \"user\", \"content\": f'''Please separate the Mean Standard Deviation (SD) into different columns inside all the following tables,\n",
    "        return all tables even they remain unchanged,only return data,no explaintions,if no table,return No table provided,here is a example\n",
    "        Example(DO NOT include this one):\n",
    "        ```json\n",
    "            {{\n",
    "            \"tables\": [\n",
    "                {{\n",
    "                \"table_title\": \"Patient Characteristics\",\n",
    "                \"is_baseline\": true,\n",
    "                \"group_ns\": {{ \"Study Group\": 50, \"Control Group\": 48 }}  // optional: integer N or null\n",
    "                \"headers\": [\"Feature\", \"Study Group\",\"Study Group Mean\",\"Study Group SD\", \"Control Group\",\"Control Group Mean\",\"Control Group SD\",\"P-value\"],\n",
    "                \"data\": [\n",
    "                    [\"Age (years)\", \"30 Â± 3.51\",\"30\", \"3.51\",\"29.2 Â± 2.93\",\"29.2\",\"2.93\", \".76\"],\n",
    "                    [\"Height (cm)\", \"158 Â± 12.33\",\"158\", \"12.33\",\"164 Â± 14.55\",\"164\",\"14.55\", \".54\"],\n",
    "                    [\"Body weight (kg)\", \"68 Â± 3.20\",\"68\", \"3.20\",\"65 Â± 4.21\",\"65\", \"4.21\",\".12\"]\n",
    "                ],\n",
    "                \"table_note\": \"Baseline characteristics of patients.\"\n",
    "                }},\n",
    "                {{\n",
    "                \"table_title\": \"Outcome 8 months after therapy\",\n",
    "                \"group_ns\": {{ \"Study Group\": 50, \"Control Group\": 48 }}  // optional: integer N or null\n",
    "                \"is_baseline\": false,\n",
    "                \"headers\": [\"Outcome\", \"Study Group\", \"Control Group\", \"P-value\"],\n",
    "                \"data\": [\n",
    "                    [\"Menstruating\", \"35 (89.6%)\", \"13 (33.3%)\", \"<.001*\"],\n",
    "                    [\"Ovulating\", \"27 (69.2%)\", \"10 (25.6%)\", \"<.001*\"],\n",
    "                    [\"POF\", \"4 (11.4%)\", \"21 (66.6%)\", \"<.001*\"]\n",
    "                ],\n",
    "                \"table_note\": \"P-value < .05 was considered statistically significant.\"\n",
    "                }}\n",
    "                ]\n",
    "            }}\n",
    "        ```\n",
    "         :\\n\\n{extract_tables}'''}\n",
    "    ]\n",
    "    messages = chat_prompt_split\n",
    "    completion = client.chat.completions.create(\n",
    "        model=AZURE_DEPLOYMENT,\n",
    "        messages=messages,\n",
    "        max_tokens=8000,\n",
    "        temperature=0.0,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "        stream=False\n",
    "    )\n",
    "    print(\"\\nðŸ“‹ Splited Table:\\n\")\n",
    "    print(completion.choices[0].message.content)\n",
    "    json_str = completion.choices[0].message.content\n",
    "    json_str = json_str.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Not JSONï¼š{e}\\nï¼š\\n{json_str}\")\n",
    "\n",
    "    output_path = \"extracted_tables.json\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# ------------------------\n",
    "# Extraction / LLM prompt improvements\n",
    "# ------------------------\n",
    "def extract_docling_data(files):\n",
    "    \"\"\"Extract tables using docling (no LLM) and return state skeleton.\"\"\"\n",
    "    if not files:\n",
    "        return [], [], \"\", {}\n",
    "    pdf_names = [Path(p).name for p in files]\n",
    "    pdf_iframes, table_names, tables = [], [], []\n",
    "    converter = DocumentConverter()\n",
    "    for p in files:\n",
    "        b64 = base64.b64encode(open(p,\"rb\").read()).decode()\n",
    "        pdf_iframes.append(f'<iframe src=\"data:application/pdf;base64,{b64}\" width=\"600\" height=\"800\" style=\"border:none;\"></iframe>')\n",
    "        res = converter.convert(p)\n",
    "        stem = Path(p).stem\n",
    "        outdir = Path(\"docling_output\")/stem\n",
    "        outdir.mkdir(exist_ok=True, parents=True)\n",
    "        names, dfs = [], []\n",
    "        for i, tbl in enumerate(res.document.tables, start=1):\n",
    "            names.append(f\"Table {i}\")\n",
    "            df = tbl.export_to_dataframe().replace(r\"\\s*/C6\\s*\", \" Â± \", regex=True)\n",
    "            dfs.append(df)\n",
    "            csv_path = outdir / f\"{stem}-table-{i}.csv\"\n",
    "            df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "        table_names.append(names)\n",
    "        tables.append(dfs)\n",
    "    state = {\n",
    "      \"pdf_names\":   pdf_names,\n",
    "      \"iframes\":     pdf_iframes,\n",
    "      \"table_names\": table_names,\n",
    "      \"tables\":      tables\n",
    "    }\n",
    "    return pdf_names, table_names[0] if table_names else [], pdf_iframes[0] if pdf_iframes else \"\", state\n",
    "\n",
    "def extract_docling_then_llm(files):\n",
    "    \"\"\"Use docling to get tables, then call LLM to split meanÂ±sd and detect group Ns.\n",
    "       Save table_ns in state parallel to state[\"tables\"].\"\"\"\n",
    "    if not files:\n",
    "        return [], [], \"\", {}\n",
    "\n",
    "    pdf_names, names_first, html_first, state = extract_docling_data(files)\n",
    "    grouped_tables = []\n",
    "    grouped_names = []\n",
    "    grouped_table_ns = []\n",
    "\n",
    "    for pdf_idx, pdf_name in enumerate(pdf_names):\n",
    "        table_json = {\"tables\": []}\n",
    "        table_names = state[\"table_names\"][pdf_idx]\n",
    "        dfs = state[\"tables\"][pdf_idx]\n",
    "        for tbl_idx, df in enumerate(dfs):\n",
    "            headers = list(df.columns)\n",
    "            data = df.values.tolist()\n",
    "            table_json[\"tables\"].append({\n",
    "                \"table_title\": f\"{pdf_name} - {table_names[tbl_idx]}\",\n",
    "                \"is_baseline\": False,\n",
    "                \"headers\": headers,\n",
    "                \"data\": data,\n",
    "                \"table_note\": \"\"\n",
    "            })\n",
    "\n",
    "        # LLM prompt: ask to split meanÂ±sd into separate columns and detect group Ns\n",
    "        prompt = [\n",
    "            {\"role\":\"system\", \"content\": \"You are a precise data extraction assistant. Return a single JSON object exactly matching the schema below.\"},\n",
    "            {\"role\":\"user\", \"content\": f\"\"\"\n",
    "Return JSON only (no commentary). Schema:\n",
    "{{\n",
    "  \"tables\": [\n",
    "    {{\n",
    "      \"table_title\": \"string\",\n",
    "      \"is_baseline\": true|false,\n",
    "      \"headers\": [\"col1\", \"GroupA Mean\", \"GroupA SD\", \"GroupB Mean\", \"GroupB SD\", ...],\n",
    "      \"data\": [[...], [...]],\n",
    "      \"table_note\": \"string\",\n",
    "      \"group_ns\": {{ \"GroupA\": 50, \"GroupB\": 48 }}  // optional: integer N or null\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "Please split any 'x Â± y' or 'x(y)' occurrences into separate mean and sd columns, update headers accordingly, and try to detect sample sizes N for each group from headers or cells (e.g., 'Group A (N=50)', '(n=50)', '35 (89.6%)'). If you cannot detect N, set it to null. Use the following raw input to produce output:\n",
    "{json.dumps(table_json, ensure_ascii=False)}\n",
    "\"\"\"}]\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=AZURE_DEPLOYMENT,\n",
    "            messages=prompt,\n",
    "            max_tokens=8000,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        json_str = completion.choices[0].message.content\n",
    "        json_str = json_str.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        data = json.loads(json_str)\n",
    "\n",
    "        pdf_tables, pdf_table_names, pdf_table_ns = [], [], []\n",
    "        for table in data.get(\"tables\", []):\n",
    "            pdf_table_names.append(table.get(\"table_title\", \"<table>\"))\n",
    "            pdf_tables.append(pd.DataFrame(table[\"data\"], columns=table[\"headers\"]))\n",
    "            gns = table.get(\"group_ns\", {}) or {}\n",
    "            normalized_gns = {str(k): (int(v) if (v is not None and str(v).strip()!='') else None) for k,v in gns.items()}\n",
    "            pdf_table_ns.append(normalized_gns)\n",
    "\n",
    "        grouped_names.append(pdf_table_names)\n",
    "        grouped_tables.append(pdf_tables)\n",
    "        grouped_table_ns.append(pdf_table_ns)\n",
    "\n",
    "    # attach to state\n",
    "    state[\"table_names\"] = grouped_names\n",
    "    state[\"tables\"] = grouped_tables\n",
    "    state[\"table_ns\"] = grouped_table_ns\n",
    "    state[\"iframes\"] = [make_pdf_iframe(f) for f in files]\n",
    "    return pdf_names, grouped_names[0], state[\"iframes\"][0], state\n",
    "\n",
    "def extract_ai_data(files):\n",
    "    \"\"\"Use image->LLM route - adapted to also expect group_ns in LLM output (similar to above).\"\"\"\n",
    "    if not files:\n",
    "        return [], [], \"\", {}\n",
    "    pdf_path = files[0]\n",
    "    b64 = base64.b64encode(open(pdf_path, \"rb\").read()).decode()\n",
    "    html_iframe = f'<iframe src=\"data:application/pdf;base64,{b64}\" width=\"600\" height=\"800\" style=\"border:none;\"></iframe>'\n",
    "    # For brevity call the same LLM prompt on image text: (you can implement image blocks like your original)\n",
    "    # Here we assume you already have a function extract_ai that returns the same JSON schema as above.\n",
    "    # We'll reuse the same JSON parsing logic: extract_ai() => json_str\n",
    "    # For this combined example, we just call a dummy extract_ai that you have in original script.\n",
    "    json_str = extract_ai(pdf_path)  # expects same output schema with group_ns\n",
    "    try:\n",
    "        data = json.loads(json_str.replace(\"```json\", \"\").replace(\"```\", \"\").strip())\n",
    "    except Exception:\n",
    "        clean = json_str.strip(\"```json\").strip(\"```\")\n",
    "        data = json.loads(clean)\n",
    "    tables = data.get(\"tables\", [])\n",
    "    names  = [t.get(\"table_title\", f\"table {i}\") for i, t in enumerate(tables)]\n",
    "    dfs    = [pd.DataFrame(t[\"data\"], columns=t[\"headers\"]) for t in tables]\n",
    "    pdf_name = Path(pdf_path).name\n",
    "    state = {\n",
    "      \"pdf_names\":    [pdf_name],\n",
    "      \"table_names\":  [names],\n",
    "      \"tables\":       [dfs],\n",
    "      \"table_ns\":     [[t.get(\"group_ns\", {}) for t in tables]],\n",
    "      \"iframes\":      [html_iframe],\n",
    "    }\n",
    "    return [pdf_name], names, html_iframe, state\n",
    "\n",
    "# ------------------------\n",
    "# Parsing helper (pair group mean/sd columns)\n",
    "# ------------------------\n",
    "def detect_group_mean_sd_headers(headers):\n",
    "    \"\"\"\n",
    "    Given headers list, detect pairs of mean/sd columns.\n",
    "    Expect header strings like '24-h group Mean', '24-h group SD'.\n",
    "    Return list of groups: [(group_label, mean_col, sd_col), ...]\n",
    "    mean_col/sd_col are header names (strings) or None.\n",
    "    \"\"\"\n",
    "    norm_map = {}\n",
    "    # normalize: remove tokens mean/sd and parentheses\n",
    "    for h in headers:\n",
    "        hstr = str(h).strip()\n",
    "        hnorm = re.sub(r'[\\(\\)\\[\\]\\{\\}]', ' ', hstr).strip()\n",
    "        low = hnorm.lower()\n",
    "        # detect role\n",
    "        if re.search(r'\\b(mean|avg|average|m|Âµ|mu)\\b', low):\n",
    "            role = 'mean'\n",
    "        elif re.search(r'\\b(sd|s\\.d|std|stddev|se|stderr)\\b', low):\n",
    "            role = 'sd'\n",
    "        else:\n",
    "            role = None\n",
    "        # compute prefix by removing role tokens\n",
    "        prefix = re.sub(r'\\b(mean|avg|average|m|Âµ|mu|sd|s\\.d|std|stddev|se|stderr)\\b', '', low).strip()\n",
    "        if prefix == '':\n",
    "            prefix = low  # fallback to full header\n",
    "        if prefix not in norm_map:\n",
    "            norm_map[prefix] = {'mean': None, 'sd': None, 'raw_prefix': prefix}\n",
    "        if role == 'mean':\n",
    "            norm_map[prefix]['mean'] = hstr\n",
    "        elif role == 'sd':\n",
    "            norm_map[prefix]['sd'] = hstr\n",
    "        else:\n",
    "            # unknown: keep as potential group header if no explicit mean/sd\n",
    "            if prefix not in norm_map:\n",
    "                norm_map[prefix] = {'mean': None, 'sd': None, 'raw_prefix': prefix}\n",
    "            # we don't set anything here\n",
    "    groups = []\n",
    "    for prefix, info in norm_map.items():\n",
    "        if info.get('mean') is not None or info.get('sd') is not None:\n",
    "            groups.append((prefix, info.get('mean'), info.get('sd')))\n",
    "    return groups\n",
    "\n",
    "# ------------------------\n",
    "# Analysis: compute per-row t (and show ranges with f_range)\n",
    "# ------------------------\n",
    "def analyze_selected_table(selected_table: str, selected_file: str, state: dict,\n",
    "                           detected_ns_json: str, assume_n_json: str, dp_p: int, show_t_flag: bool):\n",
    "    # prefer detected_ns_json (edited), else try assume_n_json\n",
    "    manual_ns_json = None\n",
    "    if detected_ns_json and detected_ns_json.strip():\n",
    "        manual_ns_json = detected_ns_json\n",
    "    elif assume_n_json and assume_n_json.strip():\n",
    "        manual_ns_json = assume_n_json\n",
    "    else:\n",
    "        manual_ns_json = None\n",
    "    \"\"\"\n",
    "    Parse currently selected table and compute per-row stats (t or F).\n",
    "    - manual_ns_json: optional editable JSON input (user override)\n",
    "    - dp_p: decimal places for rounding (-1 auto)\n",
    "    - show_t_flag: True => output t values (sqrt of F)\n",
    "    Returns:\n",
    "      - analysis_text: string summary\n",
    "      - result_df: pandas DataFrame ready to display in UI (one row per original row, with computed nominal/min/max)\n",
    "    \"\"\"\n",
    "    if not state or \"pdf_names\" not in state:\n",
    "        return \"No state present. Run extraction first.\", pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        file_idx = state[\"pdf_names\"].index(selected_file)\n",
    "    except ValueError:\n",
    "        return \"Selected file not in state.\", pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        tbl_idx = state[\"table_names\"][file_idx].index(selected_table)\n",
    "    except ValueError:\n",
    "        return \"Selected table not found in state.\", pd.DataFrame()\n",
    "\n",
    "    df = state[\"tables\"][file_idx][tbl_idx].copy()\n",
    "    headers = list(df.columns)\n",
    "\n",
    "    # 1) detect groups (mean/sd column pairs)\n",
    "    groups = detect_group_mean_sd_headers(headers)\n",
    "    if len(groups) < 2:\n",
    "        # try fallback: maybe columns are \"GroupA Mean\", \"GroupA SD\", \"GroupB Mean\", ...\n",
    "        return \"Could not detect at least two group mean/sd pairs in headers. Ensure LLM split mean and sd into separate columns.\", pd.DataFrame()\n",
    "\n",
    "    # 2) build mapping header->group name and locate mean/sd column names\n",
    "    group_labels = [g[0] for g in groups]\n",
    "    mean_cols = [g[1] for g in groups]\n",
    "    sd_cols = [g[2] for g in groups]\n",
    "\n",
    "    # 3) Load Ns: priority order\n",
    "    #   a) manual override JSON in UI (if provided)\n",
    "    #   b) LLM-provided state[\"table_ns\"]\n",
    "    #   c) try to find Ns inside cells (simple heuristics)\n",
    "    override_ns = {}\n",
    "    if manual_ns_json:\n",
    "        try:\n",
    "            parsed = json.loads(manual_ns_json)\n",
    "            if isinstance(parsed, dict):\n",
    "                override_ns = {str(k): int(v) for k, v in parsed.items()}\n",
    "        except Exception:\n",
    "            # invalid JSON - ignore and continue to other sources (we'll inform user)\n",
    "            override_ns = {}\n",
    "\n",
    "    llm_ns_map = {}\n",
    "    if \"table_ns\" in state:\n",
    "        try:\n",
    "            llm_ns_map = state[\"table_ns\"][file_idx][tbl_idx]\n",
    "        except Exception:\n",
    "            llm_ns_map = {}\n",
    "\n",
    "    # merged ns_map (fallback)\n",
    "    ns_map = {}\n",
    "    for label, mcol, scol in groups:\n",
    "        # try manual override exact match or fuzzy match\n",
    "        n_val = None\n",
    "        # exact label keys first\n",
    "        if label in override_ns:\n",
    "            n_val = int(override_ns[label])\n",
    "        else:\n",
    "            # fuzzy: try to find a key in override_ns that matches prefix\n",
    "            for k in override_ns:\n",
    "                if k.lower() in label.lower() or label.lower() in k.lower():\n",
    "                    n_val = int(override_ns[k]); break\n",
    "        if n_val is None:\n",
    "            # try LLM-provided\n",
    "            if label in llm_ns_map and llm_ns_map[label] is not None:\n",
    "                n_val = int(llm_ns_map[label])\n",
    "            else:\n",
    "                # fuzzy match\n",
    "                for k in llm_ns_map:\n",
    "                    if k.lower() in label.lower() or label.lower() in k.lower():\n",
    "                        n_val = llm_ns_map[k]\n",
    "                        if n_val is not None:\n",
    "                            n_val = int(n_val)\n",
    "                        break\n",
    "        # finally try to detect an N in mean or sd column values (like '56 (89.6%)')\n",
    "        if n_val is None:\n",
    "            for c in (mcol, scol):\n",
    "                if c is None or c not in df.columns:\n",
    "                    continue\n",
    "                vals = []\n",
    "                for cell in df[c].astype(str):\n",
    "                    if not cell or cell.strip() == \"\":\n",
    "                        continue\n",
    "                    m = re.match(r'^\\s*(\\d+)\\s*\\(\\s*[0-9\\.]+%?\\s*\\)\\s*$', cell.strip())\n",
    "                    if m:\n",
    "                        vals.append(int(m.group(1)))\n",
    "                if len(vals) > 0:\n",
    "                    # choose the most common\n",
    "                    vals_u, counts = np.unique(vals, return_counts=True)\n",
    "                    n_val = int(vals_u[np.argmax(counts)])\n",
    "                    break\n",
    "        ns_map[label] = n_val\n",
    "\n",
    "    # check any missing\n",
    "    missing_ns = [k for k, v in ns_map.items() if v is None]\n",
    "    if missing_ns:\n",
    "        # Ask user to provide Ns via manual override; present parsed preview\n",
    "        parsed_preview = {\n",
    "            \"means_headers\": mean_cols,\n",
    "            \"sd_headers\": sd_cols,\n",
    "            \"detected_ns\": ns_map,\n",
    "            \"labels\": group_labels\n",
    "        }\n",
    "        return (\"Missing sample sizes for groups: \" + \", \".join(missing_ns) +\n",
    "                \". Provide Ns in the 'Assume N' box as JSON map (e.g. {\\\"24-h group\\\":56, \\\"72-h group\\\":51}).\\n\\n\"\n",
    "                \"Parsed preview:\\n\" + json.dumps(parsed_preview, indent=2)), pd.DataFrame()\n",
    "\n",
    "    # 4) For each row, compute t (2-group case) or F (k-group case) & ranges using f_range\n",
    "    results = []\n",
    "    num_groups = len(groups)\n",
    "    for ridx, row in df.iterrows():\n",
    "        # build per-group mean and sd for this row\n",
    "        mvals = []\n",
    "        svals = []\n",
    "        nvals = []\n",
    "        for label, mcol, scol in groups:\n",
    "            # parse mean\n",
    "            mean_val = np.nan\n",
    "            sd_val = np.nan\n",
    "            # mean column may be None (rare) - try to find best candidate\n",
    "            if mcol and mcol in df.columns:\n",
    "                cell = df.at[ridx, mcol]\n",
    "                parsed = parse_mean_sd_cell(cell)\n",
    "                if parsed:\n",
    "                    mean_val = parsed[0]\n",
    "                    # if parsed sd and separate sd col also exists, prefer separate sd col\n",
    "                    if parsed[1] is not None and (not scol or scol not in df.columns):\n",
    "                        sd_val = parsed[1]\n",
    "                else:\n",
    "                    try:\n",
    "                        mean_val = float(str(cell).strip())\n",
    "                    except:\n",
    "                        mean_val = np.nan\n",
    "            # parse sd if separate column exists\n",
    "            if scol and scol in df.columns:\n",
    "                scell = df.at[ridx, scol]\n",
    "                if pd.isna(scell) or str(scell).strip()==\"\":\n",
    "                    pass\n",
    "                else:\n",
    "                    parsed2 = re.search(r'([+-]?\\d+(?:\\.\\d+)?)', str(scell))\n",
    "                    if parsed2:\n",
    "                        try:\n",
    "                            sd_val = float(parsed2.group(1))\n",
    "                        except:\n",
    "                            pass\n",
    "            # if sd still nan and mean column had sd, keep that\n",
    "            if np.isnan(sd_val):\n",
    "                # try to re-parse from mean column if not yet done\n",
    "                if mcol and mcol in df.columns:\n",
    "                    parsed = parse_mean_sd_cell(df.at[ridx, mcol])\n",
    "                    if parsed:\n",
    "                        sd_val = parsed[1]\n",
    "\n",
    "            mvals.append(mean_val)\n",
    "            svals.append(sd_val)\n",
    "            nvals.append(ns_map[label])\n",
    "\n",
    "        # call f_range for this single-row case\n",
    "        try:\n",
    "            r = f_range(m=mvals, s=svals, n=nvals, title=f\"{selected_file} - {selected_table} - row {ridx}\",\n",
    "                        show_t=show_t_flag, dp_p=dp_p)\n",
    "            # f_range returns arrays for nominal/min/max; for one-way with k groups we may get len>1 (F_A etc.)\n",
    "            # For simple 2-group one-way, r['nominal'][0] is t (if show_t=True) or F (if show_t=False)\n",
    "            nominal = r['nominal']\n",
    "            mn = r['min']\n",
    "            mx = r['max']\n",
    "            # produce row label if first column is row header\n",
    "            row_label = None\n",
    "            first_col = df.columns[0] if len(df.columns)>0 else None\n",
    "            if first_col:\n",
    "                row_label = str(df.at[ridx, first_col])\n",
    "            results.append({\n",
    "                \"row_index\": int(ridx),\n",
    "                \"row_label\": row_label,\n",
    "                \"nominal\": nominal,\n",
    "                \"min\": mn,\n",
    "                \"max\": mx,\n",
    "                \"dp_used\": r['dp'],\n",
    "                \"enumeration_mode\": r['enumeration_mode'],\n",
    "                \"total_combinations\": r['total_combinations']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"row_index\": int(ridx),\n",
    "                \"row_label\": str(df.at[ridx, df.columns[0]]) if df.shape[1]>0 else f\"row{ridx}\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "\n",
    "    # Build result DataFrame convenient for display\n",
    "    rows_out = []\n",
    "    for item in results:\n",
    "        if 'error' in item:\n",
    "            rows_out.append({\n",
    "                \"row_index\": item[\"row_index\"],\n",
    "                \"row_label\": item.get(\"row_label\"),\n",
    "                \"error\": item[\"error\"]\n",
    "            })\n",
    "            continue\n",
    "        # join arrays into readable strings (nom/min/max for each effect)\n",
    "        nominal_str = \", \".join([str(round(x,6)) if x is not None else \"NA\" for x in item[\"nominal\"]])\n",
    "        min_str = \", \".join([str(round(x,6)) if x is not None else \"NA\" for x in item[\"min\"]])\n",
    "        max_str = \", \".join([str(round(x,6)) if x is not None else \"NA\" for x in item[\"max\"]])\n",
    "        rows_out.append({\n",
    "            \"row_index\": item[\"row_index\"],\n",
    "            \"row_label\": item.get(\"row_label\"),\n",
    "            \"nominal\": nominal_str,\n",
    "            \"min\": min_str,\n",
    "            \"max\": max_str,\n",
    "            \"dp\": item[\"dp_used\"],\n",
    "            \"enumeration_mode\": item[\"enumeration_mode\"],\n",
    "            \"total_combinations\": item[\"total_combinations\"]\n",
    "        })\n",
    "\n",
    "    result_df = pd.DataFrame(rows_out)\n",
    "\n",
    "    # Compose summary text\n",
    "    summary_lines = []\n",
    "    summary_lines.append(f\"Analysis for table: {selected_table} (file: {selected_file})\")\n",
    "    summary_lines.append(f\"Groups detected (label order): {', '.join(group_labels)}\")\n",
    "    summary_lines.append(\"Note: nominal/min/max may contain multiple values (one per tested effect).\")\n",
    "    summary_text = \"\\n\".join(summary_lines)\n",
    "\n",
    "    return summary_text, result_df\n",
    "\n",
    "# ------------------------\n",
    "# Gradio UI\n",
    "# ------------------------\n",
    "def combined_extract(mode, method, files, url, title, authors):\n",
    "    # File upload methods\n",
    "    if mode==\"File Upload\" and method==\"Docling\":\n",
    "        pdf_names, names_first, html_first, state_dict = extract_docling_data(files)\n",
    "        file_update  = gr.update(choices=pdf_names, value=pdf_names[0] if pdf_names else None)\n",
    "        table_update = gr.update(choices=names_first, value=names_first[0] if names_first else None)\n",
    "        return file_update, table_update, html_first, state_dict\n",
    "\n",
    "    if mode==\"File Upload\" and method==\"Docling+LLM\":\n",
    "        pdf_names, names_first, html_first, state_dict = extract_docling_then_llm(files)\n",
    "        file_update  = gr.update(choices=pdf_names, value=pdf_names[0] if pdf_names else None)\n",
    "        table_update = gr.update(choices=names_first, value=names_first[0] if names_first else None)\n",
    "        return file_update, table_update, html_first, state_dict\n",
    "\n",
    "    if mode==\"File Upload\" and method==\"AI\":\n",
    "        file_names, table_names, html_iframe, state = extract_ai_data(files)\n",
    "        return gr.update(choices=file_names, value=file_names[0] if file_names else None), gr.update(choices=table_names, value=table_names[0] if table_names else None), html_iframe, state\n",
    "\n",
    "    if mode==\"URL Extraction\":\n",
    "        raw_json, split_json = extract_url(url)\n",
    "        state_dict = {\"raw_tables\": raw_json, \"split_tables\": split_json}\n",
    "        return gr.update(choices=[], value=None), gr.update(choices=[], value=None), \"\", state_dict\n",
    "\n",
    "    if mode==\"Metadata\":\n",
    "        raw_json, split_json = extract_meta(title, authors)\n",
    "        state_dict = {\"raw_tables\": raw_json, \"split_tables\": split_json}\n",
    "        return gr.update(choices=[], value=None), gr.update(choices=[], value=None), \"\", state_dict\n",
    "\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"## ðŸ“‘ Multimode table extraction + per-row t/F analysis (LLM-detected Ns with editable override)\")\n",
    "\n",
    "    mode    = gr.Radio([\"File Upload\",\"URL Extraction\",\"Metadata\"], value=\"File Upload\", label=\"Mode\")\n",
    "    method = gr.Radio([\"AI\", \"Docling\", \"Docling+LLM\"], value=\"Docling+LLM\", label=\"File Upload Method\")\n",
    "    files   = gr.File(file_count=\"multiple\", type=\"filepath\", label=\"Upload PDF(s)\")\n",
    "    url_in  = gr.Textbox(label=\"Document URL\")\n",
    "    title_in= gr.Textbox(label=\"Article Title\")\n",
    "    auth_in = gr.Textbox(label=\"Authors\")\n",
    "\n",
    "    file_selector  = gr.Dropdown(label=\"Select PDF\",   choices=[], value=None)\n",
    "    table_selector = gr.Dropdown(label=\"Select Table\", choices=[], value=None)\n",
    "    state = gr.State({})\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            pdf_preview = gr.HTML(label=\"PDF Preview\")\n",
    "            # show detected Ns for the selected table (LLM)\n",
    "            detected_ns_box = gr.Textbox(label=\"Detected Ns (from LLM) - click 'Load detected Ns' then edit if needed\", lines=3)\n",
    "            load_ns_btn = gr.Button(\"Load detected Ns for selected table\")\n",
    "        with gr.Column(scale=1):\n",
    "            table_view = gr.Dataframe(label=\"Extracted Table (select a row to inspect)\", interactive=True)\n",
    "            dl_trigger = gr.Button(\"Download CSV\")\n",
    "            download_btn = gr.File(label=\"\", file_count=\"single\", type=\"filepath\", visible=False)\n",
    "\n",
    "    # Analysis panel\n",
    "    gr.Markdown(\"### Analysis Controls\")\n",
    "    assume_n_input = gr.Textbox(label=\"Manual Ns override (JSON dict) e.g. {\\\"24-h group\\\":56, \\\"72-h group\\\":51}\", placeholder='{\"24-h group\":56,\"72-h group\":51}', lines=2)\n",
    "    dp_input = gr.Number(value=-1, label=\"dp (decimal places) (-1 = auto)\", precision=0)\n",
    "    show_t_checkbox = gr.Checkbox(value=True, label=\"Show t (sqrt of F)\")\n",
    "    analyze_btn = gr.Button(\"Analyze selected table\")\n",
    "    analysis_output = gr.Textbox(label=\"Analysis Summary\", lines=6)\n",
    "    analysis_df = gr.Dataframe(label=\"Per-row results\", interactive=False)\n",
    "\n",
    "    def on_download(selected_table, selected_file, st):\n",
    "        if not st:\n",
    "            return \"\"\n",
    "        fidx = st[\"pdf_names\"].index(selected_file)\n",
    "        tidx = st[\"table_names\"][fidx].index(selected_table)\n",
    "        df = st[\"tables\"][fidx][tidx]\n",
    "        csv_path = f\"{selected_file}_{selected_table}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        return csv_path\n",
    "\n",
    "    dl_trigger.click(fn=on_download, inputs=[table_selector, file_selector, state], outputs=[download_btn])\n",
    "\n",
    "    # Wire extraction button\n",
    "    extract_btn = gr.Button(\"Extract\")\n",
    "    extract_btn.click(fn=combined_extract,\n",
    "                      inputs=[mode, method, files, url_in, title_in, auth_in],\n",
    "                      outputs=[file_selector, table_selector, pdf_preview, state])\n",
    "\n",
    "    # file/table selection callbacks\n",
    "    def on_file_change(selected_file: str, st: dict):\n",
    "        if not st:\n",
    "            return gr.update(choices=[], value=None), \"\", pd.DataFrame()\n",
    "        file_idx = st[\"pdf_names\"].index(selected_file)\n",
    "        iframe_html = st[\"iframes\"][file_idx]\n",
    "        names = st[\"table_names\"][file_idx]\n",
    "        df0 = st[\"tables\"][file_idx][0] if names else pd.DataFrame()\n",
    "        return gr.update(choices=names, value=names[0] if names else None), iframe_html, df0\n",
    "\n",
    "    file_selector.change(fn=on_file_change, inputs=[file_selector, state], outputs=[table_selector, pdf_preview, table_view])\n",
    "\n",
    "    def on_table_change(selected_table: str, selected_file: str, st: dict):\n",
    "        if not st:\n",
    "            return pd.DataFrame()\n",
    "        file_idx = st[\"pdf_names\"].index(selected_file)\n",
    "        tbl_idx  = st[\"table_names\"][file_idx].index(selected_table)\n",
    "        return st[\"tables\"][file_idx][tbl_idx]\n",
    "\n",
    "    table_selector.change(fn=on_table_change, inputs=[table_selector, file_selector, state], outputs=[table_view])\n",
    "\n",
    "    # Load detected Ns into the editable textbox\n",
    "    def load_detected_ns(selected_table, selected_file, st):\n",
    "        if not st:\n",
    "            return \"\"\n",
    "        try:\n",
    "            file_idx = st[\"pdf_names\"].index(selected_file)\n",
    "            tbl_idx = st[\"table_names\"][file_idx].index(selected_table)\n",
    "            ns_map = st.get(\"table_ns\", [[{}]])[file_idx][tbl_idx]\n",
    "            return json.dumps(ns_map, indent=2)\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    load_ns_btn.click(fn=load_detected_ns, inputs=[table_selector, file_selector, state], outputs=[detected_ns_box])\n",
    "\n",
    "    # Analysis button wires to analyzer\n",
    "    analyze_btn.click(fn=analyze_selected_table,\n",
    "                  inputs=[table_selector, file_selector, state, detected_ns_box, assume_n_input, dp_input, show_t_checkbox],\n",
    "                  outputs=[analysis_output, analysis_df])\n",
    "\n",
    "    app.launch(share=True, debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
